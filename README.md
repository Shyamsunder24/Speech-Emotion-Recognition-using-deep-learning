# Speech-Emotion-Recognition-using-deep-learning
Speech Emotion Recognition (SER) is a significant area of research with diverse
applications, including human-computer interaction, affective computing, and mental
health monitoring. This paper presents a comprehensive approach to SER using machine
learning techniques, with a focus on the Toronto Emotional Speech Set (TESS) dataset,
which provides high-quality audio recordings of emotional speech from female speakers.
The proposed system involves data loading, preprocessing, feature extraction, model
design, training, and evaluation. Feature extraction is performed using Mel Frequency
Cepstral Coefficients (MFCC) to capture relevant acoustic characteristics from the audio
signals. The model architecture consists of a Long Short-Term Memory (LSTM) neural
network, capable of learning temporal dependencies in speech sequences.
The system is trained using categorical cross-entropy loss and optimized with the Adam
optimizer. Through extensive experimentation and validation, the proposed SER system
demonstrates promising performance in accurately recognizing emotions from speech
signals. The effectiveness of the system is evaluated through various metrics, including
accuracy, precision, recall, and F1-score. Overall, the proposed SER system offers a
robust framework for emotion recognition in speech, with potential applications in diverse
real-world scenarios
